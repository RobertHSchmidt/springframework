<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">
<chapter id="spring-batch-infrastructure">
  <title>ItemReaders and ItemWriters</title>

  <section>
    <title id="i-0.spring-batch-infrastructure-overview">Introduction</title>

    <para>All batch processing can be described in its most simple form as
    reading in large amounts of data, performing some type of calculation or
    transformation, and writing the result out. Spring Batch provides two key
    interfaces to help perform bulk reading and writing:
    <classname>ItemReader</classname> and
    <classname>ItemWriter</classname>.</para>
  </section>

  <section>
    <title id="infrastructure.1">ItemReader</title>

    <para>Although a simple concept, an <classname>ItemReader</classname> is
    the means for providing data from many different types of input. The most
    general examples include: <itemizedlist>
        <listitem>
          <para>Flat File- Flat File Item Readers read lines of data from a
          flat file that typically describe records with fields of data
          defined by fixed positions in the file or delimited by some special
          character (e.g. Comma).</para>
        </listitem>

        <listitem>
          <para>XML - XML ItemReaders process XML independently of
          technologies used for parsing, mapping and validating objects. Input
          data allows for the validation of and XML file against an XSD
          schema.</para>
        </listitem>

        <listitem>
          <para>Database - A database resource is accessed that returns
          resultsets which can be mapped to objects for processing. The
          default SQL ItemReaders invoke a <classname>RowMapper</classname> to
          return objects, keep track of the current row if restart is
          required, basic statistics, and some transaction enhancements that
          will be explained later.</para>
        </listitem>
      </itemizedlist>There are many more possibilities, but we'll focus on the
    basic ones for this chapter. A complete list of all available ItemReaders
    can be found in Appendix A.</para>

    <para><classname>ItemReader</classname> is a basic interface for generic
    input operations:</para>

    <programlisting>public interface ItemReader&lt;T&gt; {

  T read() throws Exception, UnexpectedInputException, ParseException;

}
</programlisting>

    <para>The <methodname>read</methodname> method defines the most essential
    contract of the <classname>ItemReader</classname>, calling it returns one
    Item, returning null if no more items are left. An item might represent a
    line in a file, a row in a database, or an element in an XML file. It is
    generally expected that these will be mapped to a usable domain object
    (i.e. Trade, Foo, etc) but there is no requirement in the contract to do
    so.</para>

    <para>It is expected that implementations of the
    <classname>ItemReader</classname> interface will be forward only. However,
    if the underlying resource is transactional (such as a JMS queue) thehn
    calling read may return the same logical item on subsequent calls in a
    rollback scenario.</para>

    <para>It is also worth noting that a lack of items to process by an
    <classname>ItemReader</classname> will not cause an exception to be
    thrown. For example, a database <classname>ItemReader</classname> that is
    configured with a query that returns 0 results will simply return null on
    the first invocation of <methodname>read</methodname>.</para>
  </section>

  <section>
    <title id="infrastructure.1.4">ItemWriter</title>

    <para><classname>ItemWriter</classname> is similar in functionality to an
    <classname>ItemReader</classname>, but with reversed operations. Resources
    still need to be located, opened and closed but they differ in that an
    <classname>ItemWriter</classname> writes out, rather than reading in. In
    the case of databases or queues these may be inserts, updates or sends.
    The format of the serialization of the output is specific for every batch
    job.</para>

    <para>As with <classname>ItemReader</classname>,
    <classname>ItemWriter</classname> is a fairly generic interface:</para>

    <programlisting>public interface ItemWriter&lt;T&gt; {

  void write(List&lt;? extends T&gt; items) throws Exception;

}
</programlisting>

    <para>As with <methodname>read</methodname> on
    <classname>ItemReader</classname>, <methodname>write</methodname> provides
    the basic contract of <classname>ItemWriter</classname>, it will attempt
    to write out the list of items passed in as long as it is open. Because it
    is generally expected that items will be 'batched' together into a chunk
    and then output, the interface accepts a list, rather than an item by
    itself. After writing out the list, any flushing that may be necessary can
    be performed before returning from the write method. For example, if
    writing to a Hibernate DAO, multiple calls to write can be made, one for
    each item. The writer can then call close on the hibernate Session before
    returning.</para>
  </section>

  <section>
    <title>ItemStream</title>

    <para>Both ItemReaders and ItemWriters serve their individual purposes
    well, but there is a common concern among both of them that necessitates
    another interface. In general, as part of the scope of a batch job,
    readers and writers need to be opened, closed, and require a mechanism for
    persisting state:</para>

    <programlisting>public interface ItemStream {

  void open(ExecutionContext executionContext) throws StreamException;

  void update(ExecutionContext executionContext);
  
  void close(ExecutionContext executionContext) throws StreamException;
}
</programlisting>

    <para>Before describing each method, its worth briefly mentioning the
    <classname>ExecutionContext</classname>. Clients of an
    <classname>ItemReader</classname> that also implements
    <classname>ItemStream</classname> should call
    <methodname>open</methodname> before any calls to
    <methodname>read</methodname>, to open any resources such as files or
    obtain connections. A similar restriction applies to an
    <classname>ItemWriter</classname> that also implements
    <classname>ItemStream</classname>. As mentioned in Chapter 2, if expected
    data is found in the <classname>ExecutionContext</classname>, it may be
    used to start the <classname>ItemReader</classname> or
    <classname>ItemWriter</classname> at a location other than its initial
    state. Conversely, <methodname>close</methodname> will be called to ensure
    any resources allocated during <methodname>open</methodname> will be
    released safely. <methodname>update</methodname> is called primarily to
    ensure that any state currently being held is loaded into the provided
    <classname>ExecutionContext</classname>. This method will be called before
    committing, to ensure that the current state is persisted in the database
    before commit.</para>

    <para>In the special case where the client of an
    <classname>ItemStream</classname> is a <classname>Step</classname> (from
    the Spring Batch Core), an <classname>ExecutionContext</classname> is
    created for each <classname>StepExecution</classname> to allow users to
    store the state of a particular execution, with the expectation that it
    will be returned if the same <classname>JobInstance</classname> is started
    again. For those familiar with Quartz, the semantics are very similar to a
    Quartz <classname>JobDataMap</classname>.</para>
  </section>

  <section>
    <title id="infrastructure.1.2">Flat Files</title>

    <para>One of the most common mechanisms for interchanging bulk data has
    always been the flat file. Unlike XML, which has an agreed upon standard
    for defining how it is structured (XSD), anyone reading a flat file must
    understand ahead of time exactly how the file is structured. In general,
    all flat files fall into two general types: Delimited and Fixed
    Length.</para>

    <section>
      <title>The FieldSet</title>

      <para>When working with flat files in Spring Batch, regardless of
      whether it is for input or output, one of the most important classes is
      the <classname>FieldSet</classname>. Many architectures and libraries
      contain abstractions for helping you read in from a file, but they
      usually return a String or an array of Strings. This really only gets
      you halfway there. A <classname>FieldSet</classname> is Spring Batchâ€™s
      abstraction for enabling the binding of fields from a file resource. It
      allows developers to work with file input in much the same way as they
      would work with database input. A <classname>FieldSet</classname> is
      conceptually very similar to a Jdbc <classname>ResultSet</classname>.
      FieldSets only require one argument, a <classname>String</classname>
      array of tokens. Optionally, you can also configure in the names of the
      fields so that the fields may be accessed either by index or name as
      patterned after <classname>ResultSet</classname>:</para>

      <programlisting>  String[] tokens = new String[]{"foo", "1", "true"};
  FieldSet fs = new DefaultFieldSet(tokens);
  String name = fs.readString(0);
  int value = fs.readInt(1);
  boolean booleanValue = fs.readBoolean(2);</programlisting>

      <para>There are many more options on the <classname>FieldSet</classname>
      interface, such as <classname>Date</classname>, long,
      <classname>BigDecimal</classname>, etc. The biggest advantage of the
      <classname>FieldSet</classname> is that it provides consistent parsing
      of flat file input. Rather than each batch job parsing differently in
      potentially unexpected ways, it can be consistent, both when handling
      errors caused by a format exception, or when doing simple data
      conversions.</para>
    </section>

    <section>
      <title id="infrastructure.1.2.1">FlatFileItemReader</title>

      <para>A flat file is any type of file that contains at most
      two-dimensional (tabular) data. Reading flat files in the Spring Batch
      framework is facilitated by the class
      <classname>FlatFileItemReader</classname>, which provides basic
      functionality for reading and parsing flat files. The three most
      important required dependencies of
      <classname>FlatFileItemReader</classname> are
      <classname>Resource</classname>, <classname>FieldSetMapper</classname>
      and <classname>LineTokenizer. </classname>The
      <classname>FieldSetMapper</classname> and
      <classname>LineTokenizer</classname> interfaces will be explored more in
      the next sections. The resource property represents a Spring Core
      <classname>Resource</classname>. Documentation explaining how to create
      beans of this type can be found in <ulink
      url="http://static.springframework.org/spring/docs/2.5.x/reference/resources.html"><citetitle>Spring
      Framework, Chapter 4.Resources</citetitle></ulink>. Therefore, this
      guide will not go into the details of creating
      <classname>Resource</classname> objects. However, a simple example of a
      file system resource can be found below: <programlisting>
        Resource resource = new FileSystemResource("resources/trades.csv");
        </programlisting></para>

      <para>In complex batch environments the directory structures are often
      managed by the EAI infrastructure where drop zones for external
      interfaces are established for moving files from ftp locations to batch
      processing locations and vice versa. File moving utilities are beyond
      the scope of the spring batch architecture but it is not unusual for
      batch job streams to include file moving utilities as steps in the job
      stream. Its sufficient that the batch architecture only needs to know
      how to locate the files to be processed. Spring Batch begins the process
      of feeding the data into the pipe from this starting point.</para>

      <para>The other properties in <classname>FlatFileItemReader</classname>
      allow you to further specify how your data will be interpreted: <table>
          <title>Flat File Item Reader Properties</title>

          <tgroup cols="3">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Property</entry>

                <entry align="center">Type</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry align="left">encoding</entry>

                <entry align="left">String</entry>

                <entry align="left">Specifies what text encoding to use -
                default is "ISO-8859-1"</entry>
              </row>

              <row>
                <entry align="left">comments</entry>

                <entry align="left">String[]</entry>

                <entry align="left">Specifies line prefixes that indicate
                comment rows</entry>
              </row>

              <row>
                <entry align="left">linesToSkip</entry>

                <entry align="left">int</entry>

                <entry align="left">Number of lines to ignore at the top of
                the file</entry>
              </row>

              <row>
                <entry align="left">firstLineIsHeader</entry>

                <entry align="left">boolean</entry>

                <entry align="left">Indicates that the first line of the file
                is a header containing field names. If the column names have
                not been set yet and the tokenizer extends
                AbstractLineTokenizer, field names will be set automatically
                from this line</entry>
              </row>

              <row>
                <entry align="left">recordSeparatorPolicy</entry>

                <entry align="left">RecordSeparatorPolicy</entry>

                <entry align="left">Used to determine where the line endings
                are and do things like continue over a line ending if inside a
                quoted string.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>

      <section>
        <title>LineMapper</title>

        <para>As with <classname>RowMapper</classname>, which takes a low
        level construct such as <classname>ResultSet and returns an Object,
        flat file procesing requires the same construct to convert a String
        line into an Object:</classname><programlisting>public interface LineMapper&lt;T&gt; {

  T mapLine(String line, int lineNumber) throws Exception;
}</programlisting></para>
      </section>

      <section>
        <title>FieldSetMapper</title>

        <para>The <classname>FieldSetMapper</classname> interface defines a
        single method, <methodname>mapLine</methodname>, which takes a
        <classname>FieldSet</classname> object and maps its contents to an
        object. This object may be a custom DTO or domain object, or it could
        be as simple as an array, depending on your needs. The
        <classname>FieldSetMapper</classname> is used in conjunction with the
        <classname>LineTokenizer</classname> to translate a line of data from
        a resource into an object of the desired type:</para>

        <programlisting>  public interface FieldSetMapper&lt;T&gt; {
  
    T mapFieldSet(FieldSet fieldSet);

  }</programlisting>

        <para>The pattern used is the same as <classname>RowMapper</classname>
        used by <classname>JdbcTemplate</classname>.</para>
      </section>

      <section>
        <title>LineTokenizer</title>

        <para>Because there can be many formats of flat file data, which all
        need to be converted to a <classname>FieldSet</classname> so that a
        <classname>FieldSetMapper</classname> can create a useful domain
        object from them, an abstraction for turning a line of input into a
        <classname>FieldSet</classname> is necessary. In Spring Batch, this is
        called a <classname>LineTokenizer</classname>:</para>

        <programlisting>  public interface LineTokenizer {
  
    FieldSet tokenize(String line);

  }</programlisting>

        <para>The contract of a <classname>LineTokenizer</classname> is such
        that, given a line of input (in theory the
        <classname>String</classname> could encompass more than one line) a
        <classname>FieldSet</classname> representing the line will be
        returned. This will then be passed to a
        <classname>FieldSetMapper</classname>. Spring Batch contains the
        following LineTokenizers:</para>

        <itemizedlist>
          <listitem>
            <para><classname>DelmitedLineTokenizer</classname> - Used for
            files that separate records by a delimiter. The most common is a
            comma, but pipes or semicolons are often used as well</para>
          </listitem>

          <listitem>
            <para><classname>FixedLengthTokenizer</classname> - Used for
            tokenizing files where each record is separated by a 'fixed width'
            that must be defined per record.</para>
          </listitem>

          <listitem>
            <para><classname>PrefixMatchingCompositeLineTokenizer</classname>
            - Tokenizer that determines which among a list of Tokenizers
            should be used on a particular line by checking against a
            prefix.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Simple Delimited File Reading Example</title>

        <para>Now that the basic interfaces for reading in flat files have
        been defined, a simple example explaining how they work together is
        helpful. In it's most simple form, the flow when reading a line from a
        file is the following:</para>

        <orderedlist>
          <listitem>
            <para>Read one line from the file.</para>
          </listitem>

          <listitem>
            <para>Pass the string line into the LineTokenizer#tokenize()
            method, in order to retrieve a
            <classname>FieldSet</classname></para>
          </listitem>

          <listitem>
            <para>Pass the FieldSet returned from tokenizing to a
            FieldSetMapper, returning the result from the ItemReader#read()
            method</para>
          </listitem>
        </orderedlist>

        <para>In code, the above flow looks like the following:</para>

        <programlisting>  String line = readLine();

  if (line != null) {
    FieldSet tokenizedLine = tokenizer.tokenize(line);
    return fieldSetMapper.mapFieldSet(tokenizedLine);
  }
 
  return null;</programlisting>

        <note>
          <para>Exception handling has been removed for clarity.</para>
        </note>

        <para>The following example will be used to illustrate this using an
        actual domain scenario. This particular batch job reads in football
        players from the following file:<programlisting>  ID,lastName,firstName,position,birthYear,debutYear
  "AbduKa00,Abdul-Jabbar,Karim,rb,1974,1996",
  "AbduRa00,Abdullah,Rabih,rb,1975,1999",
  "AberWa00,Abercrombie,Walter,rb,1959,1982",
  "AbraDa00,Abramowicz,Danny,wr,1945,1967",
  "AdamBo00,Adams,Bob,te,1946,1969",
  "AdamCh00,Adams,Charlie,wr,1979,2003"        </programlisting></para>

        <para>The contents of this file will be mapped to the following Player
        domain object: <programlisting>
  public class Player implements Serializable {
        
  private String ID; 
  private String lastName; 
  private String firstName; 
  private String position; 
  private int birthYear; 
  private int debutYear;
        
    public String toString() {
                
      return "PLAYER:ID=" + ID + ",Last Name=" + lastName + 
        ",First Name=" + firstName + ",Position=" + position + 
        ",Birth Year=" + birthYear + ",DebutYear=" + 
        debutYear;
    }
   
    // setters and getters...
  }
          </programlisting></para>

        <para>In order to map a <classname>FieldSet</classname> into a Player
        object, a <classname>FieldSetMapper</classname> that returns players
        needs to be defined:</para>

        <para><programlisting>
  protected static class PlayerFieldSetMapper implements FieldSetMapper&lt;Player&gt; {
    public Object mapLine(FieldSet fieldSet) {
      Player player = new Player();

      player.setID(fieldSet.readString(0));
      player.setLastName(fieldSet.readString(1));
      player.setFirstName(fieldSet.readString(2)); 
      player.setPosition(fieldSet.readString(3));
      player.setBirthYear(fieldSet.readInt(4));
      player.setDebutYear(fieldSet.readInt(5));

      return player;
    }
  }    </programlisting></para>

        <para>The file can then be read by correctly constructing a
        <classname>FlatFileItemReader</classname> and calling
        <methodname>read</methodname>:</para>

        <programlisting>  
  FlatFileItemReader&lt;Player&gt; itemReader = new FlatFileItemReader&lt;Player&gt;();
  itemReader.setResource(new FileSystemResource("resources/players.csv"));
  //DelimitedLineTokenizer defaults to comma as it's delimiter
  itemReader.setLineTokenizer(new DelimitedLineTokenizer());
  itemReader.setFieldSetMapper(new PlayerFieldSetMapper());
  itemReader.open(new ExecutionContext());
  Player player = itemReader.read();

</programlisting>

        <para>Each call to <methodname>read</methodname> will return a new
        Player object from each line in the file. When the end of the file is
        reached, null will be returned.</para>
      </section>

      <section>
        <title>Mapping fields by name</title>

        <para>There is one additional functionality a
        <classname>LineTokenizer</classname> that is similar in function to a
        JDBC <classname>ResultSet</classname>. The names of the fields can be
        injected into the <classname>LineTokenizer</classname> to increase the
        readability of the mapping function. First, the column names of all
        fields in the flat file are injected into the
        <classname>LineTokenizer</classname>:</para>

        <para><programlisting>
  tokenizer.setNames(new String[] {"ID", "lastName","firstName","position","birthYear","debutYear"}); 
          </programlisting></para>

        <para>a <classname>FieldSetMapper</classname> can this use this
        information as follows:</para>

        <para><programlisting>
    public class PlayerMapper implements FieldSetMapper&lt;Player&gt; {
        public Object mapLine(FieldSet fs) {
                        
           if(fs == null){
              return null;
           }
                        
           Player player = new Player();
           player.setID(fs.readString(<emphasis role="bold">"ID"</emphasis>));
           player.setLastName(fs.readString(<emphasis role="bold">"lastName"</emphasis>));
           player.setFirstName(fs.readString(<emphasis role="bold">"firstName"</emphasis>));
           player.setPosition(fs.readString(<emphasis role="bold">"position"</emphasis>));
           player.setDebutYear(fs.readInt(<emphasis role="bold">"debutYear"</emphasis>));
           player.setBirthYear(fs.readInt(<emphasis role="bold">"birthYear"</emphasis>));
                        
           return player;
        }

   }        </programlisting></para>
      </section>

      <section>
        <title>Automapping FieldSets to Domain Objects</title>

        <para>For many, having to write a specific
        <classname>FieldSetMapper</classname> is equally as cumbersome as
        writing a specific <classname>RowMapper</classname> for a
        JdbcTemplate. Spring Batch makes this easier by providing a
        <classname>FieldSetMapper</classname> that automatically maps fields
        by matching a field name with a setter on the object using the
        JavaBean spec. Again using the football example, the
        <classname>FieldSetMapper</classname> configuration looks like the
        following:</para>

        <programlisting>  &lt;bean id="fieldSetMapper"
        class="org.springframework.batch.item.file.mapping.BeanWrapperFieldSetMapper"&gt;
    &lt;property name="prototypeBeanName" value="player" /&gt;
  &lt;/bean&gt;

  &lt;bean id="player"
        class="org.springframework.batch.sample.domain.Player"
        scope="prototype" /&gt;</programlisting>

        <para>For each entry in the <classname>FieldSet</classname>, the
        mapper will look for a corresponding setter on a new instance of the
        <classname>Player</classname> object (for this reason, prototype scope
        is required) in the same way the Spring container will look for
        setters matching a property name. Each available field in the
        <classname>FieldSet</classname> will be mapped, and the resultant
        <classname>Player</classname> object will be returned, with no code
        required.</para>
      </section>

      <section>
        <title>Fixed Length file formats</title>

        <para>So far only delimited files have been discussed in much detail,
        however, they respresent only half of the file reading picture. Many
        organizations that use flat files use fixed length formats. An example
        fixed length file is below:</para>

        <programlisting>  UK21341EAH4121131.11customer1
  UK21341EAH4221232.11customer2
  UK21341EAH4321333.11customer3
  UK21341EAH4421434.11customer4
  UK21341EAH4521535.11customer5</programlisting>

        <para>While this looks like one large field, it actually represent 4
        distinct fields:</para>

        <orderedlist>
          <listitem>
            <para>ISIN: Unique identifier for the item being order - 12
            characters long.</para>
          </listitem>

          <listitem>
            <para>Quantity: Number of this item being ordered - 3 characters
            long.</para>
          </listitem>

          <listitem>
            <para>Price: Price of the item - 5 characters long.</para>
          </listitem>

          <listitem>
            <para>Customer: Id of the customer ordering the item - 9
            characters long.</para>
          </listitem>
        </orderedlist>

        <para>When configuring the
        <classname>FixedLengthLineTokenizer</classname>, each of these lengths
        must be provided in the form of ranges:</para>

        <programlisting>  
  &lt;bean id="fixedLengthLineTokenizer"
        class="org.springframework.batch.io.file.transform.FixedLengthTokenizer"&gt;
    &lt;property name="names" value="ISIN, Quantity, Price, Customer" /&gt;
    &lt;property name="columns" value="1-12, 13-15, 16-20, 21-29" /&gt;
  &lt;/bean&gt;

</programlisting>

        <para>This <classname>LineTokenizer</classname> will return the same
        <classname>FieldSet</classname> as if a dlimiter had been used,
        allowing the same approachs above to be used such as the
        <classname>BeanWrapperFieldSetMapper</classname>, in a way that is
        ignorant of how the actual line was parsed.</para>

        <para>It should be noted that supporting the above ranges requires a
        specialized property editor be configured anywhere in the
        <classname>ApplicationContext</classname>:</para>

        <programlisting>  
  &lt;bean id="customEditorConfigurer" class="org.springframework.beans.factory.config.CustomEditorConfigurer"&gt;
    &lt;property name="customEditors"&gt;
      &lt;map&gt;
        &lt;entry key="org.springframework.batch.item.file.transform.Range[]"&gt;
          &lt;bean class="org.springframework.batch.item.file.transform.RangeArrayPropertyEditor" /&gt;
        &lt;/entry&gt;
      &lt;/map&gt;
    &lt;/property&gt;
  &lt;/bean&gt;

</programlisting>
      </section>

      <section>
        <title>Multiple record types within a single file</title>

        <para>All of the file reading examples up to this point have all made
        a key assumption for simplicity's sake: one record equals one line.
        However, this may not always be the case. Its very common that a file
        might have records spanning multiple lines with multiple formats. The
        following excerpt from a file illustrates this:</para>

        <programlisting>  HEA;0013100345;2007-02-15
  NCU;Smith;Peter;;T;20014539;F
  BAD;;Oak Street 31/A;;Small Town;00235;IL;US
  SAD;Smith, Elizabeth;Elm Street 17;;Some City;30011;FL;United States
  BIN;VISA;VISA-12345678903
  LIT;1044391041;37.49;0;0;4.99;2.99;1;45.47
  LIT;2134776319;221.99;5;0;7.99;2.99;1;221.87
  SIN;UPS;EXP;DELIVER ONLY ON WEEKDAYS
  FOT;2;2;267.34</programlisting>

        <para>Everything between the line starting with 'HEA' and the line
        starting with 'FOT' is considered one record. The
        PrefixMatchingCompositeLineTokenizer makes this easier by matching the
        prefix in a line with a particular tokenizer:</para>

        <programlisting>  &lt;bean id="orderFileDescriptor"
        class="org.springframework.batch.io.file.transform.PrefixMatchingCompositeLineTokenizer"&gt;
    &lt;property name="tokenizers"&gt;
     &lt;map&gt;
      &lt;entry key="HEA" value-ref="headerRecordDescriptor" /&gt;
      &lt;entry key="FOT" value-ref="footerRecordDescriptor" /&gt;
      &lt;entry key="BCU" value-ref="businessCustomerLineDescriptor" /&gt;
      &lt;entry key="NCU" value-ref="customerLineDescriptor" /&gt;
      &lt;entry key="BAD" value-ref="billingAddressLineDescriptor" /&gt;
      &lt;entry key="SAD" value-ref="shippingAddressLineDescriptor" /&gt;
      &lt;entry key="BIN" value-ref="billingLineDescriptor" /&gt;
      &lt;entry key="SIN" value-ref="shippingLineDescriptor" /&gt;
      &lt;entry key="LIT" value-ref="itemLineDescriptor" /&gt;
      &lt;entry key="" value-ref="defaultLineDescriptor" /&gt;
     &lt;/map&gt;
    &lt;/property&gt;
  &lt;/bean&gt;</programlisting>

        <para>This ensures that the line will be parsed correctly, which is
        especially important for fixed length input. Any users of the
        <classname>FlatFileItemReader</classname> in this scenario must
        continue calling <methodname>read</methodname> until the footer for
        the record is returned, allowing them to return a complete order as
        one 'item'.</para>
      </section>

      <section>
        <title>Exception Handling in flat files</title>

        <para>There are many scenarios when tokenizing a line that cause
        exceptions to be thrown. Many flat files are imperfect and contain
        records that aren't formatted correctly. Many users choose to skip the
        lines causing these errors, logging out the issue, original line, and
        line number, for manual inspection later. (or by another batch job)
        For this reason, Spring Batch provides a hierarchy of exceptions for
        handling parse exceptions:
        <classname>FlatFileParseException</classname> and
        <classname>FlatFileFormatException</classname>.
        <classname>FlatFileParseException</classname> is thrown by the
        <classname>FlatFileItemReader</classname> when any errors are
        encountered while trying to read a file.
        <classname>FlatFileFormatException</classname> is thrown by
        implementations of the <classname>LineTokenizer</classname> interface,
        and indicates a more specific error encountered while
        tokenizing.</para>

        <section>
          <title>IncorrectTokenCountException</title>

          <para>Both <classname>DelimitedLineTokenizer</classname> and
          <classname>FixedLengthLineTokenizer</classname> have the ability to
          specify column names that can be used for creating a
          <classname>FieldSet</classname>. However, if the number of column
          names doesn't match the number of columns found while tokenizing a
          line the <classname>FieldSet</classname> can't be created, and a
          IncorrectTokenCountException is thrown, which contains the number of
          tokens encountered, and the number expected:</para>

          <programlisting>  
  tokenizer.setNames(new String[] {"A", "B", "C", "D"});
  
  try{
    tokenizer.tokenize("a,b,c");
  }
  catch(IncorrectTokenCountException e){
    assertEquals(4, e.getExpectedCount());
    assertEquals(3, e.getActualCount());
  }

</programlisting>

          <para>Because the tokenizer was configured with 4 columns, but only
          3 tokens were found in the file, an IncorrectTokenCountException was
          thrown.</para>
        </section>

        <section>
          <title>IncorrectLineLengthException</title>

          <para>Files formatted in a fixed length format have additional
          requirements when parsing because unlike a delimited format, each
          column must strictly adhere to the width defined for it. If the
          total line length doesn't add up to the widest value of this column,
          an exception is thrown: </para>

          <programlisting>  
  tokenizer.setColumns(new Range[] { new Range(1, 5), new Range(6, 10), new Range(11, 15) });
  try {
    tokenizer.tokenize("12345");
    fail("Expected IncorrectLineLengthException");
  }
  catch (IncorrectLineLengthException ex) {
    assertEquals(15, ex.getExpectedLength());
    assertEquals(5, ex.getActualLength());
  }

</programlisting>

          <para>The configured ranges for the tokenizer above are: 1-5, 6-10,
          and 11-15, thus the total length of the line expected is 15.
          However, in this case a line of length 5 was passed in, causing an
          <classname>IncorrectLineLengthException</classname> to be thrown.
          Throwing an exception here rather than only mapping the first column
          allows the processing of the line to fail earlier, and with more
          information than it would if it failed while trying to read in
          column 2 in a <classname>FieldSetMapper</classname>. However, there
          are scenarios where the length of the line isn't always constant.
          For this reason, validation of line length can be turned off via the
          'strict' property:</para>

          <programlisting>
  tokenizer.setColumns(new Range[] { new Range(1, 5), new Range(6, 10) });
  tokenizer.setStrict(false);
  FieldSet tokens = tokenizer.tokenize("12345");
  assertEquals("12345", tokens.readString(0));
  assertEquals("", tokens.readString(1));

</programlisting>

          <para>The above example is almost identical to the one before it,
          except the tokenizer.setStrict(false) was called. This setting tells
          the tokenizer to not enforce line lengths when tokenizing the line.
          A <classname>FieldSet</classname> is now correctly created and
          returned. However, it will only contain empty tokens for the
          remaining values.</para>
        </section>
      </section>
    </section>

    <section>
      <title>FlatFileItemWriter</title>

      <para>Writing out to flat files has the same problems and issues that
      reading in from a file must overcome. It must be able to write out in
      either delimited or fixed length formats in a transactional
      manner.</para>

      <section>
        <title>LineAggregator</title>

        <para>Just as the <classname>LineTokenizer</classname> interface is
        necessary to take an item and turn it into a string, file writing must
        have a way to aggregate multiple fields into a single string for
        writing to a file. In Spring Batch this is the
        <classname>LineAggregator</classname>:</para>

        <programlisting>  public interface LineAggregator&lt;T&gt; {

    public String aggregate(T item);

  }</programlisting>

        <para>The <classname>LineAggregator</classname> is the opposite of a
        <classname>LineTokenizer</classname>.
        <classname>LineTokenizer</classname> takes a
        <classname>String</classname> and returns a
        <classname>FieldSet</classname>, whereas
        <classname>LineAggregator</classname> takes an
        <classname>item</classname> and returns a
        <classname>String</classname>. As with reading there are two types:
        <classname>DelimitedLineAggregator</classname> and
        <classname>FixedLengthLineAggregator</classname>.</para>
      </section>

      <section>
        <title>Simple Delimited File Writing Example</title>

        <para>Now that the <classname>LineAggregator</classname> interface has
        been defined, the basic flow of writing can be explained:</para>

        <orderedlist>
          <listitem>
            <para>The object to be written is passed to the
            <classname>LineAggregator</classname> in order to obtain a
            <classname>String</classname>.</para>
          </listitem>

          <listitem>
            <para>The returned <classname>String</classname> is written to the
            configured file.</para>
          </listitem>
        </orderedlist>

        <para>The following excerpt from the
        <classname>FlatFileItemWriter</classname> expresses this in
        code:</para>

        <programlisting>  public void write(T item) throws Exception {
    getOutputState().write(lineAggregator.aggregate(item) + LINE_SEPARATOR);
  }</programlisting>

        <para>A simple configuration with the smallest ammount of setters
        would look like the following:</para>

        <programlisting>  &lt;bean id="itemWriter"
        class="org.springframework.batch.io.file.FlatFileItemWriter"&gt;
    &lt;property name="resource"
              value="file:target/test-outputs/20070122.testStream.multilineStep.txt" /&gt;
    &lt;property name="lineAggregator"&gt;
      &lt;bean class="org.springframework.batch.item.file.transform.PassThroughLineAggregator"/&gt;
    &lt;/property&gt;
  &lt;/bean&gt;</programlisting>
      </section>

      <section>
        <title>Handling file creation</title>

        <para><classname>FlatFileItemReader</classname> has a very simple
        relationship with file resources. When the reader is initialized, it
        opens the file if it exists, and throws an exception if it does not.
        File writing isn't quite so simple. At first glance it seems like a
        similar straight forward contract should exist for
        <classname>FlatFileItemWriter</classname>, if the file already exists,
        throw an exception, if it does not, create it and start writing.
        However, potentially restarting a <classname>Job</classname> can cause
        issues. In normal restart scenarios, the contract is reversed, if the
        file exists start writing to it from the last known good position, if
        it does not, throw an exception. However, what happens if the file
        name for this job is always the same? In this case, you would want to
        delete the file if it exists, unless it's a restart. Because of this
        possibility, the <classname>FlatFileItemWriter</classname> contains
        the property, <methodname>shouldDeleteIfExists</methodname>. Setting
        this property to true will cause an existing file with the same name
        to be deleted when the writer is opened.</para>
      </section>
    </section>
  </section>

  <section>
    <title id="infrastructure.2.3">XML Item Readers and Writers</title>

    <para>Spring Batch provides transactional infrastructure for both reading
    XML records and mapping them to Java objects as well as writing Java
    objects as XML records.</para>

    <note>
      <title>Constraints on streaming XML</title>

      <para>The StAX API is used for I/O as other standard XML parsing APIs do
      not fit batch processing requirements (DOM loads the whole input into
      memory at once and SAX controls the parsing process allowing the user
      only to provide callbacks).</para>
    </note>

    <para>Lets take a closer look how XML input and output works in Spring
    Batch. First, there are a few concepts that vary from file reading and
    writing but are common across Spring Batch XML processing. With XML
    processing, instead of lines of records (FieldSets) that need to be
    tokenized, it is assumed an XML resource is a collection of 'fragments'
    corresponding to individual records:</para>

    <para><mediaobject>
        <imageobject role="fo">
          <imagedata align="center"
                     fileref="src/site/resources/reference/images/xmlinput.png"
                     format="PNG" width="66%" />
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" fileref="images/xmlinput.png" format="PNG"
                     width="66%" />
        </imageobject>

        <caption><para>Figure 3.1: XML Input</para></caption>
      </mediaobject></para>

    <para>The 'trade' tag is defined as the 'root element' in the scenario
    above. Everything between '&lt;trade&gt;' and '&lt;/trade&gt;' is
    considered one 'fragment'. Spring Batch uses Object/XML Mapping (OXM) to
    bind fragments to objects. However, Spring Batch is not tied to any
    particular xml binding technology. Typical use is to delegate to <ulink
    url="http://static.springframework.org/spring-ws/site/reference/html/oxm.html"><citetitle>Spring
    OXM</citetitle></ulink>, which provides uniform abstraction for the most
    popular OXM technologies. The dependency on Spring OXM is optional and you
    can choose to implement Spring Batch specific interfaces if desired. The
    relationship to the technologies that OXM supports can be shown as the
    following:</para>

    <para><mediaobject>
        <imageobject role="fo">
          <imagedata align="center"
                     fileref="src/site/resources/reference/images/oxm-fragments.png"
                     format="PNG" width="66%" />
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" fileref="images/oxm-fragments.png"
                     format="PNG" width="66%" />
        </imageobject>

        <caption><para>Figure 3.2: OXM Binding</para></caption>
      </mediaobject></para>

    <para>Now with an introduction to OXM and how one can use XML fragments to
    represent records, let's take a closer look at Item Readers and Item
    Writers.</para>

    <section>
      <title>StaxEventItemReader</title>

      <para>The <classname>StaxEventItemReader</classname> configuration
      provides a typical setup for the processing of records from an XML input
      stream. First, lets examine a set of XML records that the
      <classname>StaxEventItemReader</classname> can process.</para>

      <para><programlisting>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;records&gt;
  &lt;trade xmlns="http://springframework.org/batch/sample/io/oxm/domain"&gt;
    &lt;isin&gt;XYZ0001&lt;/isin&gt;
    &lt;quantity&gt;5&lt;/quantity&gt;
    &lt;price&gt;11.39&lt;/price&gt;
    &lt;customer&gt;Customer1&lt;/customer&gt;
  &lt;/trade&gt;
  &lt;trade xmlns="http://springframework.org/batch/sample/io/oxm/domain"&gt;
    &lt;isin&gt;XYZ0002&lt;/isin&gt;
    &lt;quantity&gt;2&lt;/quantity&gt;
    &lt;price&gt;72.99&lt;/price&gt;
    &lt;customer&gt;Customer2c&lt;/customer&gt;
  &lt;/trade&gt;
  &lt;trade xmlns="http://springframework.org/batch/sample/io/oxm/domain"&gt;
    &lt;isin&gt;XYZ0003&lt;/isin&gt;
    &lt;quantity&gt;9&lt;/quantity&gt;
    &lt;price&gt;99.99&lt;/price&gt;
    &lt;customer&gt;Customer3&lt;/customer&gt;
  &lt;/trade&gt;
&lt;/records&gt;</programlisting></para>

      <para>To be able to process the XML records the following is needed:
      <itemizedlist>
          <listitem>
            <para>Root Element Name - Name of the root element of the fragment
            that constitutes the object to be mapped. The example
            configuration demonstrates this with the value of trade.</para>
          </listitem>

          <listitem>
            <para>Resource - Spring Resource that represents the file to be
            read.</para>
          </listitem>

          <listitem>
            <para><classname>FragmentDeserializer</classname> - UnMarshalling
            facility provided by Spring OXM for mapping the XML fragment to an
            object.</para>
          </listitem>
        </itemizedlist></para>

      <para><programlisting>&lt;property name="itemReader"&gt;
     &lt;bean class="org.springframework.batch.io.xml.StaxEventItemReader"&gt;
         &lt;property name="fragmentRootElementName"  value="trade" /&gt;
         &lt;property name="resource" value="data/staxJob/input/20070918.testStream.xmlFileStep.xml" /&gt;
         &lt;property name="fragmentDeserializer"&gt;
             &lt;bean class="org.springframework.batch.io.xml.oxm.UnmarshallingEventReaderDeserializer"&gt;
                 &lt;constructor-arg&gt;
                     &lt;bean class="org.springframework.oxm.xstream.XStreamMarshaller"&gt;
                         &lt;property name="aliases" ref="aliases" /&gt;
                     &lt;/bean&gt;
                 &lt;/constructor-arg&gt;
             &lt;/bean&gt;
         &lt;/property&gt;
     &lt;/bean&gt;
&lt;/property&gt;
    </programlisting></para>

      <para>Notice that in this example we have chosen to use an
      <classname>XStreamMarshaller</classname> that requires an alias passed
      in as a map with the first key and value being the name of the fragment
      (i.e. root element) and the object type to bind. Then, similar to a
      <classname>FieldSet</classname>, the names of the other elements that
      map to fields within the object type are described as key/value pairs in
      the map. In the configuration file we can use a spring configuration
      utility to describe the required alias as follows:</para>

      <para><programlisting>
        &lt;util:map id="aliases"&gt;
                &lt;entry key="trade"
                        value="org.springframework.batch.sample.domain.Trade" /&gt;
                &lt;entry key="isin" value="java.lang.String" /&gt;
                &lt;entry key="quantity" value="long" /&gt;
                &lt;entry key="price" value="java.math.BigDecimal" /&gt;
                &lt;entry key="customer" value="java.lang.String" /&gt;
        &lt;/util:map&gt;
        </programlisting></para>

      <para>On input the reader reads the XML resource until it recognizes a
      new fragment is about to start (by matching the tag name by default).
      The reader creates a standalone XML document from the fragment (or at
      least makes it appear so) and passes the document to a deserializer
      (typically a wrapper around a Spring OXM
      <classname>Unmarshaller</classname>) to map the XML to a Java
      object.</para>

      <para>In summary, if you were to see this in scripted code like Java the
      injection provided by the spring configuration would look something like
      the following:</para>

      <para><programlisting>
      StaxEventItemReader xmlStaxEventItemReader = new StaxEventItemReader()
      Resource resource = new ByteArrayResource(xmlResource.getBytes()) 

      Map aliases = new HashMap();
      aliases.put("trade","org.springframework.batch.sample.domain.Trade");
      aliases.put("isin","java.lang.String");
      aliases.put("quantity","long");
      aliases.put("price","java.math.BigDecimal");
      aliases.put("customer","java.lang.String");
      Marshaller marshaller = new XStreamMarshaller();
      marshaller.setAliases(aliases);
      xmlStaxEventItemReader.setFragmentDeserializer(new UnmarshallingEventReaderDeserializer(marshaller));
      xmlStaxEventItemReader.setResource(resource);
      xmlStaxEventItemReader.setFragmentRootElementName("trade");
      xmlStaxEventItemReader.open(new ExecutionContext());

      boolean hasNext = true
      
      while (hasNext) {
        trade = xmlStaxEventItemReader.read();
        if (trade == null) {
                hasNext = false;
        } else {
                println trade;
        }
      }

</programlisting></para>
    </section>

    <section>
      <title>StaxEventItemWriter</title>

      <para>Output works symmetrically to input. The
      <classname>StaxEventItemWriter</classname> needs a
      <classname>Resource</classname>, a serializer, and a rootTagName. A Java
      object is passed to a serializer (typically a wrapper around Spring OXM
      <classname>Marshaller</classname>) which writes to a
      <classname>Resource</classname> using a custom event writer that filters
      the <classname>StartDocument</classname> and
      <classname>EndDocument</classname> events produced for each fragment by
      the OXM tools. We'll show this in an example using the
      <classname>MarshallingEventWriterSerializer</classname>. The Spring
      configuration for this setup looks as follows:</para>

      <programlisting>&lt;bean class="org.springframework.batch.item.xml.StaxEventItemWriter" id="tradeStaxWriter"&gt;
  &lt;property name="resource"value="file:target/test-outputs/20070918.testStream.xmlFileStep.output.xml" /&gt;
  &lt;property name="serializer" ref="tradeMarshallingSerializer" /&gt;
  &lt;property name="rootTagName" value="trades" /&gt;
  &lt;property name="overwriteOutput" value="true" /&gt;
&lt;/bean&gt;
</programlisting>

      <para>The configuration sets up the three required properties and
      optionally sets the overwriteOutput=true, mentioned earlier in the
      chapter for specifying whether an existing file can be overwritten. The
      <classname>TradeMarshallingSerializer</classname> is configured as
      follows:</para>

      <programlisting><parameter>&lt;bean class="org.springframework.batch.item.xml.oxm.MarshallingEventWriterSerializer" id="tradeMarshallingSerializer"&gt;
  &lt;constructor-arg&gt;
   &lt;bean class="org.springframework.oxm.xstream.XStreamMarshaller"&gt;
     &lt;property name="aliases" ref="aliases" /&gt;
   &lt;/bean&gt;
  &lt;/constructor-arg&gt;
&lt;/bean&gt;</parameter></programlisting>

      <para>To summarize with a Java example, the following code illustrates
      all of the points discussed, demonstrating the programmatic setup of the
      required properties.</para>

      <programlisting>     StaxEventItemWriter staxItemWriter = new StaxEventItemWriter()
     FileSystemResource resource = new FileSystemResource(File.createTempFile("StaxEventWriterOutputSourceTests", "xml"))

     Map aliases = new HashMap();
     aliases.put("trade","org.springframework.batch.sample.domain.Trade");
     aliases.put("isin","java.lang.String");
     aliases.put("quantity","long");
     aliases.put("price","java.math.BigDecimal");
     aliases.put("customer","java.lang.String");
     XStreamMarshaller marshaller = new XStreamMarshaller()
     marshaller.setAliases(aliases)

     MarshallingEventWriterSerializer tradeMarshallingSerializer = new MarshallingEventWriterSerializer(marshaller)

     staxItemWriter.setResource(resource)
     staxItemWriter.setSerializer(tradeMarshallingSerializer)
     staxItemWriter.setRootTagName("trades")
     staxItemWriter.setOverwriteOutput(true)

     ExecutionContext executionContext = new ExecutionContext()
     staxItemWriter.open(executionContext)
     Trade trade = new Trade()
     trade.isin = "XYZ0001"
     trade.quantity =5 
     trade.price = 11.39 
     trade.customer = "Customer1"
     println trade
     staxItemWriter.write(trade)
     staxItemWriter.flush()</programlisting>

      <para>For a complete example configuration of XML input and output and a
      corresponding Job see the sample xmlStaxJob.</para>
    </section>
  </section>

  <section>
    <title>Creating File Names at Runtime</title>

    <para>Both the XML and Flat File examples above use the Spring
    <classname>Resource</classname> abstraction to obtain the file to read or
    write from. This works because <classname>Resource</classname> has a
    <markup>getFile</markup> method, that returns a
    <classname>java.io.File</classname>. Both XML and Flat File resources can
    be configured using standard Spring constructs:</para>

    <programlisting>  &lt;bean id="flatFileItemReader"
        class="org.springframework.batch.item.file.FlatFileItemReader"&gt;
    &lt;property name="resource"
                     value="file://outputs/20070122.testStream.CustomerReportStep.TEMP.txt" /&gt;
  &lt;/bean&gt;</programlisting>

    <para>The above <classname>Resource</classname> will load the file from
    the file system, at the location specificied. Note that absolute locations
    have to start with a double slash ("//"). In most spring applications,
    this solution is good enough because the names of these are known at
    compile time. However, in batch scenarios, the file name may need to be
    determined at runtime as a parameter to the job. This could be solved
    using '-D' parameters, i.e. a system property:</para>

    <programlisting>&lt;bean id="flatFileItemReader"
        class="org.springframework.batch.item.file.FlatFileItemReader"&gt;
    &lt;property name="resource" value="${input.file.name}" /&gt;
&lt;/bean&gt;</programlisting>

    <para>All that would be required for this solution to work would be a
    system argument (-Dinput.file.name="file://file.txt"). (Note that although
    a <classname>PropertyPlaceholderConfigurer</classname> can be used here,
    it is not necessary if the system property is always set because the
    <classname>ResourceEditor</classname> in Spring already filters and does
    placeholder replacement on system properties.)</para>

    <para>Often in a batch setting it is preferable to parameterize the file
    name in the <classname>JobParameters</classname> of the job, instead of
    through system properties, and access them that way. To allow for this,
    Spring Batch provides the
    <classname>StepExecutionResourceProxy</classname>. The proxy can use
    either job name, step name, or any values from the
    <classname>JobParameters</classname>, by surrounding them with %:</para>

    <programlisting>  &lt;bean id="inputFile"
        class="org.springframework.batch.core.resource.StepExecutionResourceProxy" /&gt;
    &lt;property name="filePattern" value="//%JOB_NAME%/%STEP_NAME%/%file.name%" /&gt;
  &lt;/bean&gt;</programlisting>

    <para>Assuming a job name of 'fooJob', and a step name of 'fooStep', and
    the key-value pair of 'file.name="fileName.txt"' is in the
    <classname>JobParameters</classname> the job is started with, the
    following filename will be passed as the <classname>Resource</classname>:
    "<filename>//fooJob/fooStep/fileName.txt</filename>". It should be noted
    that in order for the proxy to have access to the
    <classname>StepExecution</classname>, it must be registered as a
    <classname>StepListener</classname>:</para>

    <programlisting>  &lt;bean id="fooStep" parent="abstractStep"
    p:itemReader-ref="itemReader"
    p:itemWriter-ref="itemWriter"&gt;
    &lt;property name="listeners" ref="inputFile" /&gt;
  &lt;/bean&gt;</programlisting>

    <para>The <classname>StepListener</classname> interface will be discussed
    in more detail in Chapter 4. For now, it is sufficient to know that the
    proxy must be registered.</para>
  </section>

  <section>
    <title>Multi-File Input</title>

    <para>It is a common requirement to process multiple files within a single
    Step. Assuming the files are all formatted the same, the
    <classname>MultiResourceItemReader</classname> supports this type of input
    for both XML and FlatFile processing. Consider the following files in a
    directory:</para>

    <programlisting>file-1.txt  file-2.txt  ignored.txt</programlisting>

    <para>file-1.txt and file-2.txt are formatted the same and for business
    reasons should be processed together. The
    <classname>MuliResourceItemReader</classname> can be used to read in both
    files by using wildcards:</para>

    <programlisting>
  &lt;bean id="multiResourceReader" class="org.springframework.batch.item.SortedMultiResourceItemReader"&gt;
    &lt;property name="resources" value="classpath:data/multiResourceJob/input/file-*.txt" /&gt;
    &lt;property name="delegate" ref="flatFileItemReader" /&gt;
  &lt;/bean&gt;

</programlisting>

    <para>The referenced delegate is a simple
    <classname>FlatFileItemReader</classname>. The above configuration will
    read input from both files, handling rollback and restart scenarios. It
    should be noted that, as with any ItemReader, adding extra input (in this
    case a file) could cause potential issues when restarting. It is
    recommended that batch jobs work with their own individual directories
    until completed successfully.</para>
  </section>

  <section>
    <title id="infrastructure.2.2">Database</title>

    <para>Like most enterprise application styles, a database is the central
    storage mechanism for batch. However, batch differs from other application
    styles due to the sheer size of the datasets that must be worked with. The
    Spring Core <classname>JdbcTemplate</classname> illustrates this problem
    well. If you use <classname>JdbcTemplate</classname> with a
    <classname>RowMapper</classname>, the <classname>RowMapper</classname>
    will be called once for every result returned from the provided query.
    This causes few issues in scenarios where the dataset is small, but the
    large datasets often necessary for batch processing would cause any JVM to
    crash quickly. If the SQL statement returns 1 million rows, the
    <classname>RowMapper</classname> will be called 1 million times, holding
    all returned results in memory until all rows have been read. Spring Batch
    provides two types of solutions for this problem: Cursor and DrivingQuery
    ItemReaders.</para>

    <section>
      <title>Cursor Based ItemReaders</title>

      <para>Using a database cursor is generally the default approach of most
      batch developers. This is because it is the database's solution to the
      problem of 'streaming' relational data. The Java
      <classname>ResultSet</classname> class is essentially an object
      orientated mechanism for manipulating a cursor. A
      <classname>ResultSet</classname> maintains a cursor to the current row
      of data. Calling <methodname>next</methodname> on a
      <classname>ResultSet</classname> moves this cursor to the next row.
      Spring Batch cursor based ItemReaders open the a cursor on
      initialization, and move the cursor forward one row for every call to
      <methodname>read</methodname>, returning a mapped object that can be
      used for processing. The <methodname>close</methodname> method will then
      be called to ensure all resources are freed up. The Spring core
      <classname>JdbcTemplate</classname> gets around this problem by using
      the callback pattern to completely map all rows in a
      <classname>ResultSet</classname> and close before returning control back
      to the method caller. However, in batch this must wait until the step is
      complete. Below is a generic diagram of how a cursor based
      <classname>ItemReader</classname> works, and while a SQL statement is
      used as an example since it is so widely known, any technology could
      implement the basic approach:</para>

      <mediaobject>
        <imageobject role="html">
          <imagedata align="center" fileref="images/cursorExample.png"
                     width="66%" />
        </imageobject>

        <imageobject role="fo">
          <imagedata align="center"
                     fileref="src/site/resources/reference/images/cursorExample.png"
                     width="66%" />
        </imageobject>
      </mediaobject>

      <para>The example illustrates the basic pattern. Given a 'FOO' table,
      which has three columns: ID, NAME, and BAR, select all rows with an ID
      greater than one but less than 7. This puts the beginning of the cursor
      (row 1) on ID 2. The result of this row should be a completely mapped
      Foo object, calling read() again, moves the cursor to the next row,
      which is the Foo with an ID of 3. The results of these reads will be
      written out after each <methodname>read</methodname>, thus allowing the
      objects to be garbage collected. (Assuming no instance variables are
      maintaining references to them)</para>

      <section>
        <title>JdbcCursorItemReader</title>

        <para><classname>JdbcCursorItemReader</classname> is the JDBC
        implementation of the cursor based technique. It works directly with a
        <classname>ResultSet</classname> and requires a SQL statement to run
        against a connection obtained from a
        <classname>DataSource</classname>. The following database schema will
        be used as an example:</para>

        <programlisting>CREATE TABLE CUSTOMER (
 ID BIGINT IDENTITY PRIMARY KEY,  
 NAME VARCHAR(45),
 CREDIT FLOAT
);</programlisting>

        <para>Many people prefer to use a domain object for each row, so we'll
        use an implementation of the <classname>RowMapper</classname>
        interface to map a <classname>CustomerCredit</classname>
        object:</para>

        <programlisting>public class CustomerCreditRowMapper implements RowMapper {

 public static final String ID_COLUMN = "id";
 public static final String NAME_COLUMN = "name";
 public static final String CREDIT_COLUMN = "credit";

 public Object mapRow(ResultSet rs, int rowNum) throws SQLException {
        CustomerCredit customerCredit = new CustomerCredit();

        customerCredit.setId(rs.getInt(ID_COLUMN));
        customerCredit.setName(rs.getString(NAME_COLUMN));
        customerCredit.setCredit(rs.getBigDecimal(CREDIT_COLUMN));

        return customerCredit;
 }

}</programlisting>

        <para>Because <classname>JdbcTemplate</classname> is so familiar to
        users of Spring, and the <classname>JdbcCursorItemReader</classname>
        shares key interfaces with it, it's useful to see an example of how to
        read in this data with <classname>JdbcTemplate</classname>, in order
        to contrast it with the <classname>ItemReader</classname>. For the
        purposes of this example, let's assume there are 1,000 rows in the
        CUSTOMER database. The first example will be using
        <classname>JdbcTemplate</classname>:</para>

        <programlisting>  
  //For simplicity sake, assume a dataSource has already been obtained
  JdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource);
  List customerCredits = jdbcTemplate.query("SELECT ID, NAME, CREDIT from CUSTOMER", new CustomerCreditRowMapper());

</programlisting>

        <para>After running this code snippet the customerCredits list will
        contain 1,000 <classname>CustomerCredit</classname> objects. In the
        query method, a connection will be obtained from the
        <classname>DataSource</classname>, the provided SQL will be run
        against it, and the <methodname>mapRow</methodname> method will be
        called for each row in the <classname>ResultSet</classname>. Let's
        constrast this with the approach of the
        <classname>JdbcCursorItemReader</classname>:</para>

        <programlisting>  
  JdbcCursorItemReader itemReader = new JdbcCursorItemReader();
  itemReader.setDataSource(dataSource);
  itemReader.setSql("SELECT ID, NAME, CREDIT from CUSTOMER");
  itemReader.setMapper(new CustomerCreditRowMapper());
  int counter = 0;
  ExecutionContext executionContext = new ExecutionContext();
  itemReader.open(executionContext);
  Object customerCredit = new Object();
  while(customerCredit != null){
    customerCredit = itemReader.read();
    counter++;
  }
  itemReader.close(executionContext);

</programlisting>

        <para>After running this code snippet the counter will equal 1,000. If
        the code above had put the returned customerCredit into a list, the
        result would have been exactly the same as with the
        <classname>JdbcTemplate</classname> example. However, the big
        advantage of the <classname>ItemReader</classname> is that it allows
        items to be 'streamed'. The <methodname>read</methodname> method can
        be called once, and the item written out via an
        <classname>ItemWriter</classname>, and then the next item obtained via
        <methodname>read</methodname>. This allows item reading and writing to
        be done in 'chunks' and committed periodically, which is the essence
        of high performance batch processing.</para>

        <section>
          <title>Additional Properties</title>

          <para>Because there are so many varying options for opening a cursor
          in Java, there are many properties on the
          <classname>JdbcCustorItemReader</classname> that can be set:</para>

          <table>
            <title>JdbcCursorItemReader Properties</title>

            <tgroup cols="2">
              <tbody>
                <row>
                  <entry>ignoreWarnings</entry>

                  <entry>Determines whether or not SQLWarnings are logged or
                  cause an exception - default is true</entry>
                </row>

                <row>
                  <entry>fetchSize</entry>

                  <entry>Gives the JDBC driver a hint as to the number of rows
                  that should be fetched from the database when more rows are
                  needed by the <classname>ResultSet</classname> object used
                  by the ItemReader. By default, no hint is given.</entry>
                </row>

                <row>
                  <entry>maxRows</entry>

                  <entry>Sets the limit for the maximum number of rows the
                  underlying <classname>ResultSet</classname> can hold at any
                  one time.</entry>
                </row>

                <row>
                  <entry>queryTimeout</entry>

                  <entry>Sets the number of seconds the driver will wait for a
                  Statement object to execute to the given number of seconds.
                  If the limit is exceeded, a
                  <classname>DataAccessEception</classname> is thrown.
                  (consult your driver vendor documentation for
                  details).</entry>
                </row>

                <row>
                  <entry>verifyCursorPosition</entry>

                  <entry>Because the same <classname>ResultSet</classname>
                  held by the ItemReader is passed to the
                  <classname>RowMapper</classname>, it's possible for users to
                  call ResultSet.next() themselves, which could cause issues
                  with the reader's internal count. Settings this value to
                  true will cause an exception to be thrown if the cursor
                  position is not the same after the
                  <classname>RowMapper</classname> call as it was
                  before.</entry>
                </row>

                <row>
                  <entry>saveState</entry>

                  <entry>Indicates whether or not the reader's state should be
                  saved in the ExecutionContext provided by
                  ItemStream#update(ExecutionContext) The default value is
                  false.</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
      </section>

      <section>
        <title>HibernateCursorItemReader</title>

        <para>Just as normal Spring users make important decisions about
        whether or not to use ORM solutions, which affects whether or not they
        use a <classname>JdbcTemplate</classname> or a
        <classname>HibernateTemplate</classname>, Spring Batch users have the
        same options. <classname>HibernateCursorItemReader</classname> is the
        Hibernate implementation of the cursor technique. Hibernate's usage in
        batch has been fairly controversial. This has largely been because
        hibernate was originally developed to support online application
        styles. However, that doesn't mean it can't be used for batch
        processing. The easiest approach for solving this problem is to use a
        <classname>StatelessSession</classname> rather than a standard
        session. This removes all of the caching and dirty checking hibernate
        employs that can cause issues when using it in a batch scenario. For
        more information on the differences between stateless and normal
        hibernate sessions, refer to the documentation of your specific
        hibernate release. The
        <classname>HibernateCursorItemReader</classname> allows you to declare
        an HQL statement and pass in a <classname>SessionFactory</classname>,
        which will pass back one item per call to
        <methodname>read</methodname> in the same basic fashion as the
        <classname>JdbcCursorItemReader</classname>. Below is an example
        configuration using the same 'customer credit' example as the JDBC
        reader:</para>

        <programlisting>  
  HibernateCursorItemReader itemReader = new HibernateCursorItemReader();
  itemReader.setQueryString("from CustomerCredit");
  //For simplicity sake, assume sessionFactory already obtained.
  itemReader.setSessionFactory(sessionFactory);
  itemReader.setUseStatelessSession(true);
  int counter = 0;
  ExecutionContext executionContext = new ExecutionContext();
  itemReader.open(executionContext);
  Object customerCredit = new Object();
  while(customerCredit != null){
    customerCredit = itemReader.read();
    counter++;
  }
  itemReader.close(executionContext);

</programlisting>

        <para>This configured <classname>ItemReader</classname> will return
        <classname>CustomerCredit</classname> objects in the exact same manner
        as described by the <classname>JdbcCursorItemReader</classname>,
        assuming hibernate mapping files have been created correctly for the
        Customer table. The 'useStatelessSession' property defaults to true,
        but has been added here to draw attention to the ability to switch it
        on or off. It is also worth noting that the fetchSize of the
        underlying cursor can be set via the setFetchSize property.</para>
      </section>
    </section>

    <section>
      <title>Paging ItemReaders</title>

      <para>An alternative to using a database cursor is executing multiple
      queries where each query is bringing back a portion of the results. We
      refer to this portion as a page. Each query that is executed must
      specify the starting row number and the number of rows that we want
      returned for the page.</para>

      <section>
        <title>JdbcPagingItemReader</title>

        <para>One implementation of a paging <classname>ItemReader</classname>
        is the <classname>JdbcPagingItemReader</classname>. The
        <classname>JdbcPagingItemReader</classname> needs a
        <classname>PagingQueryProvider</classname> responsible for providing
        the SQL queries used to retrieve the rows making up a page. Since each
        database has its own strategy for proviing paging support, we need to
        use a different <classname>PagingQueryProvider</classname> for each
        supported database type. There is also the
        <classname>SimpleDelegatingPagingQueryProvider</classname> that will
        auto-detect the database that is being used and use that to adjust to
        use the appropriate <classname>PagingQueryProvider</classname>
        implementation. This simplifies the configuration and is the
        recommended best practice.</para>

        <para>The <classname>SimpleDelegatingPagingQueryProvider</classname>
        requires that you specify a select clause and a from clause. You can
        also provide an optional where clause. These clauses will be used to
        build an SQL statement combined with the required sortKey. </para>

        <para>After the reader has been opened, it will pass back one item per
        call to <methodname>read</methodname> in the same basic fashion as any
        other <classname>ItemReader</classname>. The paging happens behind the
        scenes when additional rows are needed. </para>

        <para>Below is an example configuration using a similar 'customer
        credit' example as the JDBC reader above:</para>

        <programlisting><![CDATA[    <bean id="itemReader"
        class="org.springframework.batch.item.database.JdbcPagingItemReader">
        <property name="dataSource" ref="dataSource"/>
        <property name="queryProvider">
            <bean class="org.springframework.batch.item.database.support.SimpleDelegatingPagingQueryProvider">
                <property name="selectClause" value="select id, name, credit"/>
                <property name="fromClause" value="from customer"/>
                <property name="whereClause" value="where status=:status"/>
                <property name="sortKey" value="id"/>
            </bean>
        </property>
        <property name="parameterValues">
            <map>
                <entry key=":status" value="NEW"/>
            </map>
        </property>
        <property name="pageSize" value="10"/>
        <property name="parameterizedRowMapper" ref="customerMapper"/>
    </bean> 
]]></programlisting>

        <para>This configured <classname>ItemReader</classname> will return
        <classname>CustomerCredit</classname> objects using the
        <classname>ParameterizedRowMapper</classname> that must be specified.
        The 'pageSize' property determines the number of entities read from
        the database for each query execution. </para>

        <para>The 'parameterValues' property can be used to specify a Map of
        parameter values for the query. If you use named parameters in the
        where clause the key for each entry should match the name of the named
        parameter. If you use a traditional '?' placeholder then the key for
        each entry should be the number of the placeholder, starting with
        1.</para>
      </section>

      <section>
        <title>JpaPagingItemReader</title>

        <para>Another implementation of a paging
        <classname>ItemReader</classname> is the
        <classname>JpaPagingItemReader</classname>. JPA doesn't have a concept
        similar to the Hibernate <classname>StatelessSession</classname> so we
        have to use other features provided by the JPA specification. SInce
        JPA supports paging, this is a natural choice when it comes to using
        JPA for batch processing. After each page is read the entities will
        become detached and the persistence context will be cleared in order
        to allow the entities to be garbage collected once the page is
        processed.</para>

        <para>The <classname>JpaPagingItemReader</classname> allows you to
        declare a JPQL statement and pass in a
        <classname>EntityManagerFactory</classname>. It will then pass back
        one item per call to <methodname>read</methodname> in the same basic
        fashion as any other <classname>ItemReader</classname>. The paging
        happens behind the scenes when additional entities are needed. Below
        is an example configuration using the same 'customer credit' example
        as the JDBC reader above:</para>

        <programlisting><![CDATA[  <bean id="itemReader"
    class="org.springframework.batch.item.database.JpaPagingItemReader">
    <property name="entityManagerFactory" ref="entityManagerFactory"/>
    <property name="queryString" value="select c from CustomerCredit c"/>
    <property name="pageSize" value="1000"/>
  </bean>
]]></programlisting>

        <para>This configured <classname>ItemReader</classname> will return
        <classname>CustomerCredit</classname> objects in the exact same manner
        as described by the <classname>JdbcPagingItemReader</classname> above,
        assuming the Customer object has the correct JPA annotations or ORM
        mapping file. The 'pageSize' property determines the number of
        entities read from the database for each query execution.</para>
      </section>
    </section>

    <section>
      <title>Driving Query Based ItemReaders</title>

      <para>In the previous section, Cursor based database input was
      discussed. However, it isn't the only option. Many database vendors,
      such as DB2, have extremely pessimistic locking strategies that can
      cause issues if the table being read also needs to be used by other
      portions of the online application. Furthermore, opening cursors over
      extremely large datasets can cause issues on certain vendors. Therefore,
      many projects prefer to use a 'Driving Query' approach to reading in
      data. This approach works by iterating over keys, rather than the entire
      object that needs to be returned, as the following example
      illustrates:</para>

      <mediaobject>
        <imageobject role="html">
          <imagedata align="center" fileref="images/drivingQueryExample.png"
                     width="66%" />
        </imageobject>

        <imageobject role="fo">
          <imagedata align="center"
                     fileref="src/site/resources/reference/images/drivingQueryExample.png"
                     width="66%" />
        </imageobject>
      </mediaobject>

      <para>As you can see, this example uses the same 'FOO' table as was used
      in the cursor based example. However, rather than selecting the entire
      row, only the ID's were selected in the SQL statement. So, rather than a
      FOO object being returned from <classname>read</classname>, an Integer
      will be returned. This number can then be used to query for the
      'details', which is a complete Foo object:</para>

      <mediaobject>
        <imageobject role="html">
          <imagedata align="center" fileref="images/drivingQueryJob.png"
                     width="66%" />
        </imageobject>

        <imageobject role="fo">
          <imagedata align="center"
                     fileref="src/site/resources/reference/images/drivingQueryJob.png"
                     width="66%" />
        </imageobject>
      </mediaobject>

      <para>As you can see, an existing DAO can be used to obtain a full 'Foo'
      object using the key obtained from the driving query. In Spring Batch,
      driving query style input is implemented with a
      <classname>DrivingQueryItemReader</classname>, which has only one
      dependency: a <classname>KeyCollector</classname></para>

      <section>
        <title>KeyCollector</title>

        <para>As the previous example illustrates, the DrivingQueryItemReader
        is fairly simple. It simply iterates over a list of keys. However, the
        real complication is how those keys are obtained. The
        <classname>KeyCollector</classname> interface abstracts this:</para>

        <programlisting>  public interface KeyCollector {

    List retrieveKeys(ExecutionContext executionContext);

    void updateContext(Object key, ExecutionContext executionContext);
  }</programlisting>

        <para>The primary method in this interface is the
        <methodname>retrieveKeys</methodname> method. It is expected that this
        method will return the keys to be processed regardless of whether or
        not it is a restart scenario. For example, if a job starts processing
        keys 1 through 1,000, and fails after processing key 500, upon
        restarting keys 500 through 1,000 should be returned. This
        functionality is made possible by the
        <methodname>updateContext</methodname> method, which saves the
        provided key (which should be the current key being processed) in the
        provided <classname>ExecutionContext</classname>. The
        <methodname>retrieveKeys</methodname> method can then use this value
        to retrieve a subset of the original keys:</para>

        <programlisting>  ExecutionContext executionContext = new ExecutionContext();
  List keys = keyStrategy.retrieveKeys(executionContext);
  //Assume keys contains 1 through 1,000
  keyStrategy.updateContext(new Long(500), executionContext);
  keys = keyStrategy.retrieveKeys(executionContext);
  //keys should now contains 500 through 1,000</programlisting>

        <para>This generalization illustrates the
        <classname>KeyCollector</classname> contract. If we assume that
        initially calling <methodname>retrieveKeys</methodname> returned 1,000
        keys (1 through 1,000), calling <methodname>updateContext</methodname>
        with key 500 should mean that calling
        <methodname>retrieveKeys</methodname> again with the same
        <classname>ExecutionContext</classname> will return 500 keys (501
        through 1,000).</para>
      </section>

      <section>
        <title>SingleColumnJdbcKeyCollector</title>

        <para>The most common driving query scenario is that of input that has
        only one column that represents its key. This is implemented as the
        <classname>SingleColumnJdbcKeyCollector</classname> class, which has
        the following options:</para>

        <table>
          <title>SinglecolumnJdbcKeyCollector properties</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>jdbcTemplate</entry>

                <entry>The JdbcTemplate to be used to query the
                database</entry>
              </row>

              <row>
                <entry>sql</entry>

                <entry>The sql statement to query the database with. It should
                return only one value.</entry>
              </row>

              <row>
                <entry>restartSql</entry>

                <entry>The sql statement to use in the case of restart.
                Because only one key will be used, this query should require
                only one argument.</entry>
              </row>

              <row>
                <entry>keyMapper</entry>

                <entry>The RowMapper implementation to be used to map the keys
                to objects. By default, this is a Spring Core
                SingleColumnRowMapper, which maps them to well known types
                such as Integer, String, etc. For more information, check the
                documentation of your specific Spring release.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>The following code helps illustrate how to setup and use a
        <classname>SingleColumnJdbcKeyCollector</classname>:</para>

        <programlisting>  SingleColumnJdbcKeyCollector keyCollector = new SingleColumnJdbcKeyCollector(getJdbcTemplate(),
  "SELECT ID from T_FOOS order by ID");

  keyCollector.setRestartSql("SELECT ID from T_FOOS where ID &gt; ? order by ID");

  ExecutionContext executionContext = new ExecutionContext();

  List keys = keyStrategy.retrieveKeys(new ExecutionContext());

  for (int i = 0; i &lt; keys.size(); i++) {
    System.out.println(keys.get(i));
  }</programlisting>

        <para>If this code were run in the proper environment with the correct
        database tables setup, then it would output the following:</para>

        <programlisting>1
2
3
4
5</programlisting>

        <para>Now, let's modify the code slightly to show what would happen if
        the code were started again after a restart, having failed after
        processing key 3 successfully:</para>

        <programlisting>  SingleColumnJdbcKeyCollector keyCollector = new SingleColumnJdbcKeyCollector(getJdbcTemplate(),
  "SELECT ID from T_FOOS order by ID");

  keyCollector.setRestartSql("SELECT ID from T_FOOS where ID &gt; ? order by ID");

  ExecutionContext executionContext = new ExecutionContext();  

  keyStrategy.updateContext(new Long(3), executionContext);
  
  List keys = keyStrategy.retrieveKeys(executionContext);

  for (int i = 0; i &lt; keys.size(); i++) {
    System.out.println(keys.get(i));
  }</programlisting>

        <para>Running this code snippet would result in the following:</para>

        <programlisting>4
5</programlisting>

        <para>The key difference between the two examples is the following
        line:</para>

        <programlisting>  keyStrategy.updateContext(new Long(3), executionContext);</programlisting>

        <para>This tells the key collector to update the provided
        <classname>ExecutionContext</classname> with the key of three. This
        will normally be called by the
        <classname>DrivingQueryItemReader</classname>, but is called directly
        for simplicities sake. By calling
        <methodname>retrieveKeys</methodname> with the
        <classname>ExecutionContext</classname> that was updated to contain 3,
        the argument of 3 will be passed to the restartSql:</para>

        <programlisting>  keyCollector.setRestartSql("SELECT ID from T_FOOS where ID &gt; ? order by ID");</programlisting>

        <para>This will cause only keys 4 and 5 to be returned, since they are
        the only ones with an ID greater than 3.</para>
      </section>

      <section>
        <title>Mapping multiple column keys</title>

        <para>The <classname>SingleColumnJdbcKeyCollector</classname> is
        extremely useful for generating keys, but only if one column uniquely
        identifies your record. What if more than one column is required to be
        able to uniquely identify your record? This should be a minority
        scenario, but it is still possible. In this case, the
        <classname>MultipleColumnJdbcKeyCollector</classname> should be used.
        It allows for mapping multiple columns by sacrificing simplicity. The
        properties needed to use the multiple column collector are the same as
        the single column version except one difference: instead of a regular
        <classname>RowMaper</classname>, an
        <classname>ExecutionContextRowMapper</classname> must be provided.
        Just like the single column version, it requires a normal SQL
        statement and a restart SQL statement. However, because the restart
        SQL statement will require more than one argument, there needs to be
        more complex handling of how keys are mapped to an execution context.
        An <classname>ExecutionContextRowMapper</classname> provides
        this:</para>

        <programlisting>public interface ExecutionContextRowMapper extends RowMapper {

  public void mapKeys(Object key, ExecutionContext executionContext);

  public PreparedStatementSetter createSetter(ExecutionContext executionContext);
}
</programlisting>

        <para>The <classname>ExecutionContextRowMapper</classname> interface
        extends the standard <classname>RowMapper</classname> interface to
        allow for multiple keys to be stored in an
        <classname>ExecutionContext</classname>, and a
        <classname>PreparedStatementSetter</classname> be created so that
        arguments to a the restart SQL statement can be set for the key
        returned.</para>

        <para>By default a implementation of the
        <classname>ExecutionContextRowMapper</classname> that uses a
        <classname>Map</classname> will be used. It is recommended that this
        implementation not be overridden. However, if a specific type of key
        needs to be returned, then a new implementation can be
        provided.</para>
      </section>

      <section>
        <title>iBatisKeyCollector</title>

        <para>Jdbc is not the only option available for key collectors, iBatis
        can be used as well. The usage of iBatis doesn't change the basic
        requirements of a <classname>KeyCollector</classname>: query, restart
        query, and <classname>DataSource</classname>. However, because iBatis
        is used, both queries are simply iBatis query ids, and the data source
        is a <classname>SqlMapClient</classname>.</para>
      </section>
    </section>

    <section>
      <title>Database ItemWriters</title>

      <para>While both Flat Files and XML have specific ItemWriters, there is
      no exact equivalent in the database world. This is because transactions
      provide all the functionality that is needed. ItemWriters are necessary
      for files because they must act as if they're transactional, keeping
      track of written items and flushing or clearing at the appropriate
      times. Databases have no need for this functionality, since the write is
      already contained in a transaction. Users can create their own DAO's
      that implement the <classname>ItemWriter</classname> interface or use
      one from a custom <classname>ItemWriter</classname> that's written for
      generic processing concerns, either way, they should work without any
      issues. The one exception to this is buffered output. This is most
      common when using hibernate as an <classname>ItemWriter</classname>, but
      could have the same issues when using Jdbc batch mode. Buffering
      database output doesn't have any inherent flaws, assuming there are no
      errors in the data. However, any errors while writing out can cause
      issues because there is no way to know which individual item caused an
      exception, as illustrated below:</para>

      <para><mediaobject>
          <imageobject role="html">
            <imagedata align="center" fileref="images/errorOnFlush.jpg" />
          </imageobject>

          <imageobject role="fo">
            <imagedata align="center"
                       fileref="src/site/resources/reference/images/errorOnFlush.jpg" />
          </imageobject>
        </mediaobject>If items are buffered before being written out, any
      errors encountered will not be thrown until the buffer is flushed just
      before a commit. For example, let's assume that 20 items will be written
      per chunk, and the 15th item throws a DataIntegrityViolationException.
      As far as the Step is concerned, all 20 item will be written out
      successfully, since there's no way to know that an error will occur
      until they are actually written out. Once
      <classname>ItemWriter#</classname><methodname>flush</methodname>() is
      called, the buffer will be emptied and the exception will be hit. At
      this point, there's nothing the <classname>Step</classname> can do, the
      transaction must be rolled back. Normally, this exception will cause the
      Item to be skipped (depending upon the skip/retry policies), and then it
      won't be written out again. However, in this scenario, there's no way
      for it to know which item caused the issue, the whole buffer was being
      written out when the failure happened. The only way to solve this issue
      is to flush after each item:</para>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/errorOnWrite.jpg" />
        </imageobject>

        <imageobject role="fo">
          <imagedata align="center"
                     fileref="src/site/resources/reference/images/errorOnWrite.jpg" />
        </imageobject>
      </mediaobject>

      <para>Because this is a common enough use case, especially when using
      Hibernate, Spring Batch provides an implementation to help:
      <classname>HibernateAwareItemWriter</classname>. The
      <classname>HibernateAwareItemWriter</classname> solves the problem in a
      straightforward way: if a chunk fails the first time, on subsequent runs
      it will be flushed after after each time. This effectively lowers the
      commit interval to one for the length of the chunk. Doing so allows for
      items to be skipped reliably. The following example illustrates how to
      configure the <classname>HibernateAwareItemWriter</classname>:</para>

      <programlisting>  &lt;bean id="hibernateItemWriter"
        class="org.springframework.batch.item.database.HibernateAwareItemWriter"&gt;
    &lt;property name="sessionFactory" ref="sessionFactory" /&gt;
    &lt;property name="delegate" ref="customerCreditWriter" /&gt;
  &lt;/bean&gt;

  &lt;bean id="customerCreditWriter"
        class="org.springframework.batch.sample.dao.HibernateCreditDao"&gt;
    &lt;property name="sessionFactory" ref="sessionFactory" /&gt;
  &lt;/bean&gt;

</programlisting>

      <para>If you are using JPA then the
      <classname>JpaAwareItemWriter</classname> provides comparable
      functionality. The following example illustrates how to configure the
      <classname>JpaAwareItemWriter</classname>:</para>

      <programlisting>  &lt;bean id="hibernateItemWriter"
        class="org.springframework.batch.item.database.JpaAwareItemWriter"&gt;
    &lt;property name="entityManagerFactory" ref="entityManagerFactory" /&gt;
    &lt;property name="delegate" ref="customerCreditWriter" /&gt;
  &lt;/bean&gt;

  &lt;bean id="customerCreditWriter"
        class="org.springframework.batch.sample.dao.JpaCreditDao"&gt;
    &lt;property name="entityManagerFactory" ref="entityManagerFactory" /&gt;
  &lt;/bean&gt;

</programlisting>
    </section>
  </section>

  <section>
    <title>Reusing Existing Services</title>

    <para>Batch systems are often used in conjunction with other application
    styles. The most common is an online system, but it may also support
    integration or even a thick client application by moving necessary bulk
    data that each application style uses. For this reason, it is common that
    many users want to reuse existing DAOs or other services within their
    batch jobs. The Spring container itself makes this fairly easy by allowing
    any necessary class to be injected. However, there may be cases where the
    existing service needs to act as an <classname>ItemReader</classname> or
    <classname>ItemWriter</classname>, either to satisfy the dependency of
    another Spring Batch class, or because it truly is the main
    <classname>ItemReader</classname> for a step. Its fairly trivial to write
    an adaptor class for each service that needs wrapping, but because its
    such a common concern, Spring Batch provides implementations:
    <classname>ItemReaderAdapter</classname> and
    <classname>ItemWriterAdapter</classname>. Both classes implement the
    standard Spring method invoking delegator pattern and are fairly simple to
    set up. Below is an example of the reader:</para>

    <programlisting>  &lt;bean id="itemReader" class="org.springframework.batch.item.adapter.ItemReaderAdapter"&gt;
    &lt;property name="targetObject" ref="fooService" /&gt;
    &lt;property name="targetMethod" value="generateFoo" /&gt;
  &lt;/bean&gt;

  &lt;bean id="fooService" class="org.springframework.batch.item.sample.FooService" /&gt;</programlisting>

    <para>One important point to note is that the contract of the targetMethod
    must be the same as the contract for <methodname>read</methodname>: when
    exhausted it will return null, otherwise an <classname>Object</classname>.
    Anything else will prevent the framework from correctly knowing when
    processing should end, either causing an infinite loop or incorrect
    failure, depending upon the implementation of the
    <classname>ItemWriter</classname>. The <classname>ItemWriter</classname>
    implementation is equally as simple:</para>

    <programlisting>  &lt;bean id="itemWriter" class="org.springframework.batch.item.adapter.ItemWriterAdapter"&gt;
    &lt;property name="targetObject" ref="fooService" /&gt;
    &lt;property name="targetMethod" value="processFoo" /&gt;
  &lt;/bean&gt;

  &lt;bean id="fooService" class="org.springframework.batch.item.sample.FooService" /&gt;
</programlisting>
  </section>

  <section>
    <title>Item Transforming</title>

    <para>The <classname>ItemReader</classname> and
    <classname>ItemWriter</classname> interfaces have been discussed in detail
    in this chapter, but what if you want to insert business logic before
    writing? One option for both reading and writing is to use the composite
    pattern: create an <classname>ItemWriter</classname> that contains another
    <classname>ItemWriter</classname>, or an <classname>ItemReader</classname>
    that contains another <classname>ItemReader</classname>. For
    example:</para>

    <programlisting>  public class CompositeItemWriter&lt;T&gt; implements ItemWriter&lt;T&gt; {

    ItemWriter&lt;T&gt; itemWriter;

    public CompositeItemWriter(ItemWriter&lt;T&gt; itemWriter) {
      this.itemWriter = itemWriter;
    }

    public void write(T item) throws Exception {

      //Add business logic here

      itemWriter.write(item);
    }

    public void clear() throws ClearFailedException {
      itemWriter.clear();
    }

    public void flush() throws FlushFailedException {
      itemWriter.flush();
    }
}</programlisting>

    <para>The class above contains another <classname>ItemWriter</classname>
    that it delgates to after having provided some business logic. It should
    be noted that the <methodname>clear</methodname> and
    <methodname>flush</methodname> methods must be propogated as well so that
    the delegate <classname>ItemWriter</classname> is notified. This pattern
    could easily be used for an <classname>ItemReader</classname> as well,
    perhaps to obtain more reference data based upon the input that was
    provided by the main <classname>ItemReader</classname>. This pattern is
    very useful if you need to control the call to
    <classname>write</classname> yourself. However, if you only want to
    'transform' the item passed in for writing before it is actual written,
    there isn't much need to call <methodname>write</methodname> yourself, you
    just want to modify the item. For this scenario, Spring Batch provides the
    <classname>ItemTransformer</classname> interface:</para>

    <programlisting>  public interface ItemProcessor&lt;I, O&gt; {

  O process(I item) throws Exception;
}</programlisting>

    <para>An <classname>ItemTransformer</classname> is very simple, given one
    object, transorm it and return another. The object provided may or may not
    be of the same type. The point is that business logic may be applied
    within transform, and is completely up to the developer to create. An
    <classname>ItemTransformer</classname> is used as part of the
    <classname>ItemTransformerItemWriter</classname>, which accepts an
    <classname>ItemWriter</classname> and an
    <classname>ItemTransformer</classname>, passing the item first to the
    transformer, before writing it. For example, assuming an
    <classname>ItemReader</classname> provides a class of type Foo, and it
    needs to be converted to type Bar before being written out. An
    <classname>ItemTransformer</classname> can be written that performs the
    conversion:</para>

    <programlisting>  public class Foo {}

  public class Bar {
    public Bar(Foo foo) {}
  }

  public class FooTransformer implements ItemProcessor{

    //Perform simple transformation, convert a Foo to a Bar
    public Object transform(Object item) throws Exception {
      assertTrue(item instanceof Foo);
      Foo foo = (Foo)item;
      return new Bar(foo);
    }
  }

  public class BarWriter implements ItemWriter{

    public void write(Object item) throws Exception {
      assertTrue(item instanceof Bar);
    }

    //rest of class ommitted for clarity
  }</programlisting>

    <para>In the very simple example above, there is a class
    <classname>Foo</classname>, a class <classname>Bar</classname>, and a
    class <classname>FooTransformer</classname> that adheres to the
    <classname>ItemTransformer</classname> interface. The transformation is
    simple, but any type of transformation could be done here. The
    <classname>BarWriter</classname> will be used to write out 'Bars',
    throwing an exception if any other type is provided. Similarly, the
    FooTransformer will throw an exception if anything but a
    <classname>Foo</classname> is provided. An
    <classname>ItemTransformerItemWriter</classname> can then be used like a
    normal ItemWriter. It will be passed a <classname>Foo</classname> for
    writing, which will be passed to the transformer, and a
    <classname>Bar</classname> returned. The resulting
    <classname>Bar</classname> will then be written:</para>

    <programlisting>  ItemTransformerItemWriter itemTransformerItemWriter = new ItemTransformerItemWriter();
  itemTransformerItemWriter.setItemTransformer(new FooTransformer());
  itemTransformerItemWriter.setDelegate(new BarWriter());
  itemTransformerItemWriter.write(new Foo());</programlisting>

    <section>
      <title>The Delegate Pattern and Registering with the Step</title>

      <para>Note that the <classname>ItemTransformerItemWriter</classname> and
      the <classname>CompositeItemWriter</classname> are examples of a
      delegation pattern, which is common in Spring Batch. The delegates
      themselves might implement callback interfaces like
      <classname>ItemStream</classname> or
      <classname>StepListener</classname>. If they do, and they are being used
      in conjunction with Spring Batch Core as part of a
      <classname>Step</classname> in a <classname>Job</classname>, then they
      almost certainly need to be registered manually with the
      <classname>Step</classname>. Registration is automatic when using the
      factory beans (<classname>*StepFactoryBean</classname>) , but only for
      the <classname>ItemReader</classname> and
      <classname>ItemWriter</classname> injected directly. The delegates are
      not known to the <classname>Step</classname>, so they need to be
      injected as listeners or streams (or both if appropriate).</para>
    </section>

    <section>
      <title>Chaining ItemTransformers</title>

      <para>Performing a single transformation is useful in many scenarios,
      but what if you want to 'chain' together multiple ItemTransformers? This
      can be accomplished using a
      <classname>CompositeItemTransformer</classname>. To update the previous,
      single transformation, example, <classname>Foo</classname> will be
      Transformed to <classname>Bar</classname>, which will be transformed to
      <classname>Foobar</classname> and written out:</para>

      <programlisting>  public class Foo {}

  public class Bar {
    public Bar(Foo foo) {}
  }

  public class Foobar{
    public Foobar(Bar bar){}
  }

  public class FooTransformer implements ItemTransformer{

    //Perform simple transformation, convert a Foo to a Bar
    public Object transform(Object item) throws Exception {
      assertTrue(item instanceof Foo);
      Foo foo = (Foo)item;
      return new Bar(foo);
    }
  }

  public class BarTransformer implements ItemTransformer{

    public Object transform(Object item) throws Exception {
      assertTrue(item instanceof Bar);
      return new Foobar((Bar)item);
    }
  }

  public class FoobarWriter implements ItemWriter{

    public void write(Object item) throws Exception {
      assertTrue(item instanceof Foobar);
    }
 
    //rest of class ommitted for clarity
  }</programlisting>

      <para>A <classname>FooTransformer</classname> and
      <classname>BarTransformer</classname> can be 'chained' together to give
      the resultant <classname>Foobar</classname>:</para>

      <programlisting>  CompositeItemProcessor compositeTransformer = new CompositeItemProcessor();
  List itemTransformers = new ArrayList();
  itemTransformers.add(new FooTransformer());
  itemTransformers.add(new BarTransformer());
  compositeTransformer.setItemTransformers(itemTransformers);</programlisting>

      <para>The compositeTransformer could be said to accept a
      <classname>Foo</classname> and return a <classname>Foobar</classname>.
      Clients of the composite transformer don't need to know that there are
      actually two separate transformations taking place. By updating the
      example from above to use the composite transformer, the correct class
      can be passed to <classname>FoobarWriter</classname>:</para>

      <programlisting>  ItemTransformerItemWriter itemTransformerItemWriter = new ItemTransformerItemWriter();
  <emphasis role="bold">itemTransformerItemWriter.setItemTransformer(compositeTransformer);</emphasis>
  itemTransformerItemWriter.setDelegate(new FoobarWriter());
  itemTransformerItemWriter.write(new Foo());</programlisting>
    </section>
  </section>

  <section>
    <title id="infrastructure.5">Validating Input</title>

    <para>During the course of this chapter, multiple approaches to parsing
    input have been discussed. Each major implementation will throw exception
    if it is not 'well-formed'. The
    <classname>FixedLengthTokenizer</classname> will throw an exception if a
    range of data is missing. Similarly, attempting to access an index in a
    <classname>RowMapper</classname> of <classname>FieldSetMapper</classname>
    that doesn't exist or is in a different format than the one expected will
    cause an exception to be thrown. All of these types of exceptions will be
    thrown before <methodname>read</methodname> returns. However, they don't
    address the issue of whether or not the returned item is valid. For
    example, if one of the fields is an age, it obviously cannot be negative.
    It will parse correctly, because it existed and is a number, but it won't
    cause an exception. Since there are already a plethora of Validation
    frameworks, Spring Batch does not attempt to provide yet another, but
    rather provides a very simple interface that can be implemented by any
    number of frameworks:</para>

    <programlisting>  public interface Validator {
  
    void validate(Object value) throws ValidationException;

  }</programlisting>

    <para>The contract is that the <methodname>validate</methodname> method
    will throw an exception if the object is invalid, and return normally if
    it is valid. Spring Batch provides an out of the box
    <classname>ItemReader</classname> that delegates to another
    <classname>ItemReader</classname> and validates the returned item:</para>

    <programlisting>  &lt;bean class="org.springframework.batch.item.validator.ValidatingItemReader"&gt;
    &lt;property name="itemReader"&gt;
      &lt;bean class="org.springframework.batch.sample.item.reader.OrderItemReader" /&gt;
    &lt;/property&gt;
    &lt;property name="validator" ref="validator" /&gt;
  &lt;/bean&gt;

  &lt;bean id="validator"
        class="org.springframework.batch.item.validator.SpringValidator"&gt;
    &lt;property name="validator"&gt;
      &lt;bean id="orderValidator"
            class="org.springmodules.validation.valang.ValangValidator"&gt;
        &lt;property name="valang"&gt;
          &lt;value&gt;
            &lt;![CDATA[
              { orderId : ? &gt; 0 AND ? &lt;= 9999999999 : 'Incorrect order ID' : 'error.order.id' }
              { totalLines : ? = size(lineItems) : 'Bad count of order lines' : 'error.order.lines.badcount'}
              { customer.registered : customer.businessCustomer = FALSE OR ? = TRUE : 'Business customer must be registered' : 'error.customer.registration'}
              { customer.companyName : customer.businessCustomer = FALSE OR ? HAS TEXT : 'Company name for business customer is mandatory' :'error.customer.companyname'}
          ]]&gt;
          &lt;/value&gt;
        &lt;/property&gt;
      &lt;/bean&gt;
    &lt;/property&gt;
  &lt;/bean&gt;
</programlisting>

    <para>This simple example shows a simple
    <classname>ValangValidator</classname> that is used to validate an order
    object. The intent is not to show Valang functionality as much as to show
    how a validator could be added.</para>

    <section>
      <title>The Delegate Pattern and Registering with the Step</title>

      <para>Note that the <classname>ValidatingItemReader</classname> is
      another example of a delegation pattern, and the delegates themselves
      might implement callback interfaces like
      <classname>ItemStream</classname> or
      <classname>StepListener</classname>. If they do, and they are being used
      in conjunction with Spring Batch Core as part of a step in a job, then
      they almost certainly need to be registered manually with the
      <classname>Step</classname>. Registration is automatic when using the
      factory beans (<classname>*StepFactoryBean</classname>) , but only for
      the <classname>ItemReader</classname> and
      <classname>ItemWriter</classname> injected directly - the delegates are
      not known to the step, so they need to be injected as listeners or
      streams (or both if appropriate).</para>
    </section>
  </section>

  <section>
    <title>Preventing state persistence</title>

    <para>By default, all of the <classname>ItemReader</classname> and
    <classname>ItemWriter</classname> implementations store their current
    state in the <classname>ExecutionContext</classname> before it is
    committed. However, this may not always be the desired behavior. For
    example, many developers choose to make their database readers
    'rerunnable' by using a process indicator. An extra column is added to the
    input data to indicate whether or not it has been processed. When a
    particular record is being read (or written out) the processed flag is
    flipped from false to true. The SQL statement can then contain an extra
    statement in the where clause, such as: "where PROCESSED_IND = false",
    thereby insuring that only unprocessed records will be returned in the
    case of a restart. In this scenario, it is prefereable to not store any
    state, such as the current row number, since it will be irrelevant upon
    restart. For this reason, all readers and writers include the 'saveState'
    property:</para>

    <programlisting>
  &lt;bean id="playerSummarizationSource"
        class="org.springframework.batch.item.database.JdbcCursorItemReader"&gt;
    &lt;property name="dataSource" ref="dataSource" /&gt;
    &lt;property name="mapper"&gt;
      &lt;bean class="org.springframework.batch.sample.mapping.PlayerSummaryMapper" /&gt;
    &lt;/property&gt;
    <emphasis role="bold">&lt;property name="saveState" value="false" /&gt;</emphasis>
    &lt;property name="sql"&gt;
      &lt;value&gt;
      SELECT games.player_id, games.year_no, SUM(COMPLETES),
      SUM(ATTEMPTS), SUM(PASSING_YARDS), SUM(PASSING_TD),
      SUM(INTERCEPTIONS), SUM(RUSHES), SUM(RUSH_YARDS),
      SUM(RECEPTIONS), SUM(RECEPTIONS_YARDS), SUM(TOTAL_TD)
      from games, players where players.player_id =
      games.player_id group by games.player_id, games.year_no
      &lt;/value&gt;
    &lt;/property&gt;
  &lt;/bean&gt;

</programlisting>

    <para>The <classname>ItemReader</classname> configured above will not make
    any entries in the <classname>ExecutionContext</classname> for any
    executions it participates in.</para>
  </section>

  <section>
    <title id="infrastructure.1.1">Creating Custom ItemReaders and
    ItemWriters</title>

    <para>So far in this chapter the basic contracts that exist for reading
    and writing in Spring Batch and some common implementations have been
    discussed. However, these are all fairly generic, and there are many
    potential scenarios that may not be covered by out of the box
    implementations. This section will show, using a simple example, how to
    create a custom <classname>ItemReader</classname> and
    <classname>ItemWriter</classname> implementation and implement their
    contracts correctly. The <classname>ItemReader</classname> will also
    implement <classname>ItemStream</classname>, in order to illustrate how to
    make a reader or writer restartable.</para>

    <section>
      <title>Custom ItemReader Example</title>

      <para>For the purpose of this example, a simple
      <classname>ItemReader</classname> implementation that reads from a
      provided list will be created. We'll start out by implementing the most
      basic contract of <classname>ItemReader</classname>,
      <methodname>read</methodname>:</para>

      <programlisting>  public class CustomItemReader&lt;T&gt; implements ItemReader&lt;T&gt;{

    List&lt;T&gt; items;

    public CustomItemReader(List&lt;T&gt; items) {
      this.items = items;
    }

    public T read() throws Exception, UnexpectedInputException,
       NoWorkFoundException, ParseException {

      if (!items.isEmpty()) {
        return items.remove(0);
      }
      return null;
    }
  }</programlisting>

      <para>This very simple class takes a list of items, and returns one at a
      time, removing it from the list. When the list empty, it returns null,
      thus satisfying the most basic requirements of an
      <classname>ItemReader</classname>, as illustrated below:</para>

      <programlisting>  List&lt;String&gt; items = new ArrayList&lt;String&gt;();
  items.add("1");
  items.add("2");
  items.add("3");

  ItemReader itemReader = new CustomItemReader&lt;String&gt;(items);
  assertEquals("1", itemReader.read());
  assertEquals("2", itemReader.read());
  assertEquals("3", itemReader.read());
  assertNull(itemReader.read());</programlisting>

      <section>
        <title>Making the <classname>ItemReader</classname>
        restartable</title>

        <para>The final challenge now is to make the
        <classname>ItemReader</classname> restartable. Currently, if the power
        goes out, and processing begins again, the
        <classname>ItemReader</classname> must start at the beginning. This is
        actually valid in many scenarios, but it is sometimes preferable that
        a batch job starts off at where it left off. The key discriminant is
        often whether the reader is stateful or stateless. A stateless reader
        does not need to worry about restartablility, but a stateful one has
        to try and reconstitute its last known state on restart. For this
        reason, we recommend that you keep custom readers stateless as far as
        possible, so you don't have to worry about restartability.</para>

        <para>If you do need to store state, then in Spring Batch, this is
        implemented with the <classname>ItemStream</classname>
        interface:</para>

        <programlisting>  public class CustomItemReader&lt;T&gt; implements ItemReader&lt;T&gt;, ItemStream {

    List&lt;T&gt; items;
    int currentIndex = 0;
    private static final String CURRENT_INDEX = "current.index";

    public CustomItemReader(List&lt;T&gt; items) {
      this.items = items;
    }

    public T read() throws Exception, UnexpectedInputException,
        ParseException {

      if (currentIndex &lt; items.size()) {
        return items.get(currentIndex++);
      }
      
      return null;
    }

    public void open(ExecutionContext executionContext) throws ItemStreamException {
      if(executionContext.containsKey(CURRENT_INDEX)){
        currentIndex = new Long(executionContext.getLong(CURRENT_INDEX)).intValue();
      }
      else{
        currentIndex = 0;
      }
    }

    public void close(ExecutionContext executionContext) throws ItemStreamException {}

    public void update(ExecutionContext executionContext) throws ItemStreamException {
      executionContext.putLong(CURRENT_INDEX, new Long(currentIndex).longValue());
    };
  }</programlisting>

        <para>On each call to <classname>ItemStream</classname>
        <methodname>update</methodname> method, the current index of the
        <classname>ItemReader</classname> will be stored in the provided
        <classname>ExecutionContext</classname> with a key of 'current.index'.
        When the <classname>ItemStream</classname> <classname>open</classname>
        method is called, the <classname>ExecutionContext</classname> is
        checked to see if it contains an entry with that key, and if so the
        current index is moved to that location. This is a fairly trivial
        example, but it still meets the general contract:</para>

        <programlisting>  ExecutionContext executionContext = new ExecutionContext();
  ((ItemStream)itemReader).open(executionContext);
  assertEquals("1", itemReader.read());
  ((ItemStream)itemReader).update(executionContext);

  List&lt;String&gt; items = new ArrayList&lt;String&gt;();
  items.add("1");
  items.add("2");
  items.add("3");
  itemReader = new CustomItemReader&lt;String&gt;(items);

  ((ItemStream)itemReader).open(executionContext);
  assertEquals("2", itemReader.read());</programlisting>

        <para>Most ItemReaders have much more sophisticated restart logic. The
        <classname>DrivingQueryItemReader</classname>, for example, only loads
        up the remaining keys to be processed, rather than loading all of them
        and then moving to the correct index.</para>

        <para>It is also worth noting that the key used within the
        <classname>ExecutionContext</classname> should not be trivial. That is
        because the same <classname>ExecutionContext</classname> is used for
        all ItemStreams within a <classname>Step</classname>. In most cases,
        simply prepending the key with the class name should be enough to
        guarantee uniqueness. However, in the rare cases where two of the same
        type of <classname>ItemStream</classname> are used in the same step
        (which can happen if two files are need for output) then a more unique
        name will be needed. For this reason, many of the Spring Batch
        ItemReader and ItemWriters have a setName() property that allows this
        key name to be overridden.</para>
      </section>
    </section>

    <section>
      <title>Custom ItemWriter Example</title>

      <para>Implementing a Custom <classname>ItemWriter</classname> is similar
      in many ways to the <classname>ItemReader</classname> example above, but
      differs in enough ways as to warrant its own example. However, adding
      restartability is essentially the same, so it won't be covered in this
      example. As with the <classname>ItemReader</classname> example, a List
      will be used in order to keep the example as simple as possible:</para>

      <programlisting>  public class CustomItemWriter&lt;T&gt; implements ItemWriter&lt;T&gt; {

    List&lt;T&gt; output = TransactionAwareProxyFactory.createTransactionalList();

    public void write(List&lt;? extends T&gt; items) throws Exception {
      output.addAll(items);
    }

    public List&lt;T&gt; getOutput() {
      return output;
    }
  }</programlisting>

      <section>
        <title>Making the <classname>ItemWriter</classname>
        restartable</title>

        <para>To make the ItemWriter restartable we would follow the same
        process as for the <classname>ItemReader</classname>, adding and
        implementing the <classname>ItemStream</classname> interface to
        synchronize the execution context. In the example we might have to
        count the number of items processed and add that as a footer record.
        If we needed to do that, we could implement
        <classname>ItemStream</classname> in our
        <classname>ItemWriter</classname> so that the counter was
        reconstituted from the execution context if the stream was
        re-opened.</para>

        <para>In many realistic cases, custom ItemWriters also delegate to
        another writer that itself is restartable (e.g. when writing to a
        file), or else it writes to a transactional resource so doesn't need
        to be restartable because it is stateless. When you have a stateful
        writer you should probably also be sure to implement
        <classname>ItemStream</classname> as well as
        <classname>ItemWriter</classname>. Remember also that the client of
        the writer needs to be aware of the <classname>ItemStream</classname>,
        so you may need to register it with a factory bean (e.g. one of the
        <classname>StepFactoryBean</classname> implementations in Spring Batch
        Core).</para>
      </section>
    </section>
  </section>
</chapter>